{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Processing Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch workflow involves taking a job and breaking it down into a series of discrete tasks. As an example, consider the typical daily morning routine many people have for getting ready to go to work or school. A typical morning routine involves multiple tasks that must be executed in a certain order. The following is a list is a notional morning routine broken down into discrete tasks.\n",
    "\n",
    "- Wake up\n",
    "- Exercise\n",
    "- Get out of bed\n",
    "- Make bed\n",
    "- Eat breakfast\n",
    "- Brush teeth\n",
    "- Take a shower\n",
    "- Get dressed\n",
    "- Comb hair\n",
    "- Listen to a news podcast\n",
    "- Lock the doors\n",
    "- Drive to work\n",
    "- Arrive at work\n",
    "\n",
    "Some of these tasks depend on the successful execution of other tasks. All of these tasks depend on waking up in the morning. As such, waking up is the first task in the workflow. Other tasks have an obvious order. It makes logical sense that you would shower after exercising and that you would probably want to brush your teeth after you eat breakfast. Other task dependencies are less strict. You can listen to your news podcast any time after you wake up. Likewise, you can make your bed anytime before locking the doors and driving to work. \n",
    "\n",
    "Now that we have a series of tasks, we wanted to create a workflow that describes the dependencies between each task. Typically, we define a batch workflow in terms of a directed acyclic graph (DAG) of tasks. In computer science, a graph is a collection of nodes connected by vertices. In the case of a workflow, the nodes are tasks and the connections are task dependencies. \n",
    "\n",
    "Workflows are *directed* graphs because the task dependencies have a specific order In the morning routine example, the task *make bed* depends on completing the *get out of bed* task. The following show *undirected* and *directed* graphs representing the relationships between the two tasks. \n",
    "\n",
    "*Undirected Graph of Two Tasks*\n",
    "\n",
    "[![Undirected Graph of Two Tasks](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcbiAgICBBW01ha2UgQmVkXSAtLS0gQltHZXQgT3V0IG9mIEJlZF07XG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVERcbiAgICBBW01ha2UgQmVkXSAtLS0gQltHZXQgT3V0IG9mIEJlZF07XG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCJ9LCJ1cGRhdGVFZGl0b3IiOmZhbHNlfQ)\n",
    "\n",
    "*Directed Graph of Two Tasks*\n",
    "\n",
    "[![Directed Graph of Two Tasks](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVEQ7XG4gICAgQVtNYWtlIEJlZF0tLT5CW0dldCBPdXQgb2YgQmVkXTtcbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVEQ7XG4gICAgQVtNYWtlIEJlZF0tLT5CW0dldCBPdXQgb2YgQmVkXTtcbiIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)\n",
    "\n",
    "Finally, workflows are *acyclic*, meaning that the workflow graph does not contain any loops. A loop occurs when two tasks mutually depend on each other. The instructions on many shampoo bottles, \"lather, rinse, repeat\", is an example of a cyclic workflow. If you literally followed these instructions, you would start by lathering your hair, then rinsing your hair, followed by repeating the cycle for all eternity. While any sane human being would stop this cycle at a certain point, computers tend to take such instructions quite literally. As a consequence, workflow engines avoid jobs with cyclic workflows. \n",
    "\n",
    "\n",
    "*Example of a Cyclic Workflow*\n",
    "\n",
    "[![Lather, Rinse, Repeat](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVEQ7XG4gICAgTGF0aGVyLS0-Umluc2U7XG4gICAgUmluc2UtLT5MYXRoZXI7IiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVEQ7XG4gICAgTGF0aGVyLS0-Umluc2U7XG4gICAgUmluc2UtLT5MYXRoZXI7IiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0)\n",
    "\n",
    "With definitions out of the way, we can take our morning routine and define a DAG-based workflow. When describing the dependencies between tasks, we do not need to define every dependency between every task. For instance, we know that we need to wake up before we exercise and that we need to exercise before we shower. Therefore, we only need to define exercising as a dependency for showering and not waking up. We assume if exercising is a dependency for showering, then we will execute all of the tasks necessary for exercising before showering.\n",
    "\n",
    "The following is the DAG corresponding to our morning routine workflow. \n",
    "\n",
    "[![DAG of Morning Routine](https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVEQ7XG5BW1dha2UgVXBdLS0-QltHZXQgb3V0IG9mIEJlZF07XG5CLS0tPkNbRXhlcmNpc2VdO1xuQi0tPkRbTWFrZSBCZWRdO1xuQy0tPkVbVGFrZSBhIFNob3dlcl07XG5FLS0-RltHZXQgRHJlc3NlZF07XG5DLS0-R1tFYXQgQnJlYWtmYXN0XTtcbkctLT5IW0JydXNoIFRlZXRoXTtcbkYtLT5JW0NvbWIgSGFpcl07XG5CLS0-SltMaXN0ZW4gdG8gTmV3cyBQb2RjYXN0XTtcbkktLT5LW0xvY2sgdGhlIERvb3JzXTtcbkQtLT5LO1xuSC0tPks7XG5LLS0-TFtEcml2ZSB0byBXb3JrXTtcbkwtLT5NW0Fycml2ZSBhdCBXb3JrXTtcbkotLT5NOyIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)](https://mermaid-js.github.io/mermaid-live-editor/#/edit/eyJjb2RlIjoiZ3JhcGggVEQ7XG5BW1dha2UgVXBdLS0-QltHZXQgb3V0IG9mIEJlZF07XG5CLS0tPkNbRXhlcmNpc2VdO1xuQi0tPkRbTWFrZSBCZWRdO1xuQy0tPkVbVGFrZSBhIFNob3dlcl07XG5FLS0-RltHZXQgRHJlc3NlZF07XG5DLS0-R1tFYXQgQnJlYWtmYXN0XTtcbkctLT5IW0JydXNoIFRlZXRoXTtcbkYtLT5JW0NvbWIgSGFpcl07XG5CLS0-SltMaXN0ZW4gdG8gTmV3cyBQb2RjYXN0XTtcbkktLT5LW0xvY2sgdGhlIERvb3JzXTtcbkQtLT5LO1xuSC0tPks7XG5LLS0-TFtEcml2ZSB0byBXb3JrXTtcbkwtLT5NW0Fycml2ZSBhdCBXb3JrXTtcbkotLT5NOyIsIm1lcm1haWQiOnsidGhlbWUiOiJkZWZhdWx0In0sInVwZGF0ZUVkaXRvciI6ZmFsc2V9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignment 6\n",
    "\n",
    "Directed acyclic graphs (DAGs) are critical when designing both batch and streaming workflows. From Cookiecutter Data Science's section on [Analysis is a DAG](https://drivendata.github.io/cookiecutter-data-science/#analysis-is-a-dag): \n",
    "\n",
    "> Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already (and you have stored the output somewhere like the data/interim directory), you don't want to wait to rerun them every time. \n",
    "\n",
    "DAG-based workflows are even more critical when multiple independent tasks use the output of a prior task. For example, an organization may want to build a single, clean dataset for use in building machine learning models. \n",
    "\n",
    "This assignment will design a simple workflow to build a dataset for use in training machine learning models. We make this dataset weekly based on demographics data from the Census, daily sales, and weather forecasts. \n",
    "\n",
    "The following are the task and dataset names and descriptions you will need to complete the assignment. \n",
    "\n",
    "***Tasks***\n",
    "\n",
    "- **AggSalesTask**: Aggregate the weekly sales data. Task runs on the first day of a new week on data from the previous week. \n",
    "- **LatestDemoTask**: Extract demographics information for the store locations from the latest Census data. Task runs when there are updates to Census data.  \n",
    "- **AggWeatherTask**: Aggregate the weekly weather for the store locations from the forecasts. Task runs on the first day of a new week on data from the previous week. \n",
    "- **ModelingDataTask**: Creates a dataset for use in building machine learning models. \n",
    "\n",
    "***Datasets***\n",
    "\n",
    "- **SalesData[0-6]**: The sales data for each day of the week where `SalesData0` is data from Monday, `SalesData1` is from Tuesday, ending with `SalesData6` being the data from Sunday. \n",
    "- **ForecastData[0-6]**: The weather data for each day of the week where `ForecastData0` is data from Monday, `ForecastData1` is from Tuesday, ending with `ForecastData6` being the data from Sunday. \n",
    "- **LatestPUMSData**: The latest PUMS data.\n",
    "- **LatestTigerData**: The latest Tiger data. \n",
    "- **LatestACSData**: The latest ACS Summary File data.\n",
    "- **AggSalesData**: The aggregated sales data output from the `AggSalesTask`.\n",
    "- **AggWeatherData**: The aggregated weather data output from the `AggWeatherTask`. \n",
    "- **LatestDemoData**: The latest demographics output from the `LatestDemoTask`.\n",
    "- **ModelingData**: Dataset for use in building machine learning models. Output from the `ModelingDataTask`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6.1\n",
    "\n",
    "There are multiple tools for creating and running DAG-based workflows including Python-focused tools such as [Paver](http://paver.github.io/paver/#), [Luigi](http://luigi.readthedocs.org/en/stable/index.html), [Airflow](https://airflow.apache.org/index.html), [Snakemake](https://snakemake.readthedocs.io/en/stable/), [Ruffus](http://www.ruffus.org.uk/), or [Joblib](https://pythonhosted.org/joblib/memory.html)). [Oozie](http://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/) are other open source workflow tools focused on Hadoop. \n",
    "\n",
    "While these tools are helpful when implementing production workflows, we will not be using any of them in this assignment to avoid unnecessary complexity. Instead, we will design the workflow as DAG using the [NetworkX](https://networkx.org/documentation/stable/)  library. The [NetworkX tutorial](https://networkx.org/documentation/stable/tutorial.html) and [NetworkX introduction](https://networkx.org/documentation/stable/reference/introduction.html) provide helpful guides to working with graphs, nodes, and edges. \n",
    "\n",
    "We represent the overall workflow as a NetworkX [DiGraph](https://networkx.org/documentation/stable/reference/classes/digraph.html). We use *nodes* to define tasks and datasets and use *edges* to represent the relationship between tasks and datasets. Because a workflow is a directed graph, the edge direction is essential.  If an edge starts at a task and ends at a dataset, the dataset is an output of the task. If an edge starts at a dataset and ends at a task, the dataset is a task input. \n",
    "\n",
    "The following code creates the NetworkX-based workflow and defines variables for the tasks and datasets. You will use these variables when adding tasks, datasets, and their relationships to the workflow graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Represents the overall DAG workflow\n",
    "workflow_graph = nx.DiGraph()\n",
    "\n",
    "# Represents the task to aggregrate the weekly sales dataset\n",
    "agg_sales_task = 'AggSalesTask'\n",
    "\n",
    "# Represents the task to create the latest demographics dataset\n",
    "latest_demo_task = 'LatestDemoTask'\n",
    "\n",
    "# Represents the task to aggregrate the weekly weather dataset\n",
    "agg_weather_task = 'AggWeatherTask'\n",
    "\n",
    "# Represents the task to create the weekly modeling dataset\n",
    "modeling_data_task = 'ModelingTask'\n",
    "\n",
    "# Represents the latest ACS Summary File data\n",
    "latest_acs_data = 'LatestACSData'\n",
    "\n",
    "# Represents the latest PUMS data\n",
    "latest_pums_data = 'LatestPUMSData'\n",
    "\n",
    "# Represents the latest Tiger data\n",
    "latest_tiger_data = 'LatestTigerData'\n",
    "\n",
    "# Represents the sales data for each day\n",
    "sales_data0 = 'SalesData0'\n",
    "sales_data1 = 'SalesData1'\n",
    "sales_data2 = 'SalesData2'\n",
    "sales_data3 = 'SalesData3'\n",
    "sales_data4 = 'SalesData4'\n",
    "sales_data5 = 'SalesData5'\n",
    "sales_data6 = 'SalesData6'\n",
    "\n",
    "# A collection containing the weekly sales data\n",
    "sales_data = [\n",
    "    sales_data0,\n",
    "    sales_data1,\n",
    "    sales_data2,\n",
    "    sales_data3,\n",
    "    sales_data4,\n",
    "    sales_data5,\n",
    "    sales_data6\n",
    "]\n",
    "\n",
    "# Represents the forecast data for each day\n",
    "forecast_data0 = 'ForecastData0'\n",
    "forecast_data1 = 'ForecastData1'\n",
    "forecast_data2 = 'ForecastData2'\n",
    "forecast_data3 = 'ForecastData3'\n",
    "forecast_data4 = 'ForecastData4'\n",
    "forecast_data5 = 'ForecastData5'\n",
    "forecast_data6 = 'ForecastData6'\n",
    "\n",
    "# A collection containing the weekly forecast data\n",
    "forecast_data = [\n",
    "    forecast_data0,\n",
    "    forecast_data1,\n",
    "    forecast_data2,\n",
    "    forecast_data3,\n",
    "    forecast_data4,\n",
    "    forecast_data5,\n",
    "    forecast_data6\n",
    "]\n",
    "\n",
    "# Represents the aggregated sales dataset\n",
    "agg_sales_data = 'AggSalesData'\n",
    "\n",
    "# Represents the aggregated weather dataset\n",
    "agg_weather_data = 'AggWeatherData'\n",
    "\n",
    "# Represents the latest demographic dataset\n",
    "latest_demo_data = 'LatestDemoData'\n",
    "\n",
    "# Represents the weekly modeling dataset\n",
    "modeling_data = 'ModelingData'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.1.a\n",
    "\n",
    "Use `workflow_graph.add_node` to add the tasks and datasets to `workflow_graph`.  For task nodes, add the attribute `dtype='task'`. For dataset nodes, add the attribute `dtype='data'`.  The following is an example of adding `agg_sales_data` as a node with the attribute `dtype='data'`. \n",
    "\n",
    "```python\n",
    "workflow_graph.add_node(agg_sales_data, dtype='data')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the sales data nodes with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the forecast data nodes with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the latest PUMS data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the latest ACS data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the latest Tiger data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the aggregated sales data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the aggregated weather data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the latest demographic data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the modeling data node with dtype='data' attribute\n",
    "\n",
    "# TODO: Add the aggregate sales task node with dtype='task' attribute\n",
    "\n",
    "# TODO: Add the latest demographics data task node with dtype='task' attribute\n",
    "\n",
    "# TODO: Add the aggregate weather task node with dtype='task' attribute\n",
    "\n",
    "# TODO: Add the modeling data task node with dtype='task' attribute\n",
    "\n",
    "# Prints the number of nodes and edges in the graph\n",
    "print('Number of nodes: {}, Number of edges: {}'.format(\n",
    "    len(workflow_graph.nodes), len(workflow_graph.edges)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.1.b\n",
    "\n",
    "Go through each task and add the dataset inputs and outputs. The following is an example of adding `sales_data0` as a dependency to `agg_sales_task`. \n",
    "\n",
    "```python\n",
    "workflow_graph.add_edge(sales_data0, agg_sales_task)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add all the inputs for `agg_sales_task`\n",
    "\n",
    "# TODO: Add the output for `agg_sales_task`\n",
    "\n",
    "# TODO: Add all the inputs for `agg_weather_task`\n",
    "\n",
    "# TODO: Add the output for `agg_weather_task`\n",
    "\n",
    "# TODO: Add all the inputs for `latest_demo_task`\n",
    "\n",
    "# TODO: Add the outputs for `latest_demo_task`\n",
    "\n",
    "# TODO: Add all the inputs for `modeling_data_task`\n",
    "\n",
    "# TODO: Add the output for `modeling_data_task`\n",
    "\n",
    "# Prints the number of nodes and edges in the graph\n",
    "print('Number of nodes: {}, Number of edges: {}'.format(\n",
    "    len(workflow_graph.nodes), len(workflow_graph.edges)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `nx.draw(workflow_graph)` to draw the graph without labels. See [NetworkX's Gallery](https://networkx.org/documentation/stable/auto_examples/index.html) for more complicated drawing examples, including an example using [custom node icons](https://networkx.org/documentation/stable/auto_examples/drawing/plot_custom_node_icons.html#sphx-glr-auto-examples-drawing-plot-custom-node-icons-py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(workflow_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 6.2\n",
    "\n",
    "In the second part of the assignment, you will analyze the workflow graph using a few of NetworkX's built-in graph analysis algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.2.a\n",
    "\n",
    "Use the following code to check for cycles in your workflow. The code should output \"Good Result: No cycles found in your workflow!\" if your workflow does not have any cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run this code to check your workflow for cycles\n",
    "\n",
    "if nx.is_directed_acyclic_graph(workflow_graph):\n",
    "    print('Good Result: No cycles found in your workflow!')  \n",
    "else:\n",
    "    print('Bad Result: Cycles found in your workflow!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.2.b\n",
    "\n",
    "Use the [ancestors](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.dag.ancestors.html) function to examine the dependencies for creating the `agg_sales_data` dataset.  This function should output all the tasks and datasets necessary to build the `agg_sales_data` dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find all the dependencies for the aggregated sales dataset\n",
    "\n",
    "agg_sales_data_deps = {}\n",
    "agg_sales_data_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.2.c\n",
    "\n",
    "Use the [ancestors](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.dag.ancestors.html) function to examine the dependencies for creating the `latest_demo_data` dataset.  This function should output the tasks and datasets necessary to build the `latest_demo_data` dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find all the dependencies for the latest demographics dataset\n",
    "\n",
    "last_demo_data_deps = {}\n",
    "last_demo_data_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.2.d\n",
    "\n",
    "Finally, use the [ancestors](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.dag.ancestors.html) function to examine the dependencies for creating the `modeling_data` dataset. Filter the output so that it only includes nodes task nodes. You can use `workflow_graph.nodes.data()` to include the data when interating over all the nodes in the graph. The following code example prints the node name and the node's data dictionary. \n",
    "\n",
    "```python\n",
    "for node_name, data_dict in workflow_graph.nodes.data():\n",
    "    print(node_name, data_dict)\n",
    "```\n",
    "\n",
    "The following creates a set containing all the task nodes in the workflow. Use the `task_nodes` set when filtering the `modeling_data` dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_nodes = {\n",
    "    node_name\n",
    "    for node_name, data_dict in workflow_graph.nodes.data()\n",
    "    if data_dict['dtype'] == 'task'\n",
    "}\n",
    "\n",
    "task_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find all the task dependencies for the modeling data set\n",
    "\n",
    "modeling_data_task_deps = {}\n",
    "modeling_data_task_deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 6.2.e\n",
    "\n",
    "Finally, verify that there are no tasks or datasets in the workflow that depend on the `modeling_data` dataset using the [descendants](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.dag.descendants.html#networkx.algorithms.dag.descendants) function. Using `descendants` on `modeling_data` should return an empty set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Verify that there are no tasks or datasets in the workflow that depend on `modeling_data`\n",
    "\n",
    "modeling_data_descendants = {}\n",
    "modeling_data_descendants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
