{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Big Data Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Assignment 2\n",
    "\n",
    "Big data is a rapidly evolving, ever-changing field. Keeping track of the latest big data stacks, programming libraries, software, and other tools requires constant vigilance. Any book on big data will be out of date by the time it is published. We need a resource that is updated on a more frequent basis. \n",
    "\n",
    "This assignment will help create that resource by researching the latest big data tools and technologies. We will use this research to create an *Awesome Big Data* list. Below is a list of similar *awesome* lists that may be useful when creating our *Awesome Big Data* list. \n",
    "\n",
    "*[Awesome Python](https://awesome-python.com/)* is a curated list of awesome Python frameworks, libraries, software and resources. It was inspired by [awesome-php](https://github.com/ziadoz/awesome-php). \n",
    "\n",
    "*[Awesome Jupyter](https://github.com/markusschanta/awesome-jupyter)* is a curated list of awesome Jupyter projects, libraries and resources. Jupyter is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations and narrative text.\n",
    "\n",
    "*[Awesome Dash](https://github.com/ucg8j/awesome-dash)* is a curated list of awesome Dash (plotly) resources. Dash is a productive Python framework for building web applications. Written on top of Flask, Plotly.js, and React.js, Dash is ideal for building data visualization apps with highly custom user interfaces in pure Python. It's particularly suited for anyone who works with data in Python.\n",
    "\n",
    "*[Awesome JavaScript](https://github.com/sorrycc/awesome-javascript)* is a collection of awesome browser-side JavaScript libraries, resources and shiny things. The [data visualization section](https://github.com/sorrycc/awesome-javascript#data-visualization) may be of use. \n",
    "\n",
    "*[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)* is a curated list of awesome Deep Learning tutorials, projects and communities.\n",
    "\n",
    "*[Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)* is a curated list of awesome machine learning frameworks, libraries and software (by language).\n",
    "\n",
    "*[Awesome Data Engineering](https://github.com/igorbarinov/awesome-data-engineering)* is a curated list of data engineering tools for software developers. \n",
    "\n",
    "*[Awesome Public Datasets](https://github.com/awesomedata/awesome-public-datasets)* is a list of a topic-centric public data sources in high quality. They are collected and tidied from blogs, answers, and user responses. \n",
    "\n",
    "*[Awesome](https://github.com/sindresorhus/awesome)* is a list of awesome lists about all kinds of interesting topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.1\n",
    "\n",
    "Before we get started, we will access your knowledge of big data by taking the [Pokémon or Big Data Quiz](http://pixelastic.github.io/pokemonorbigdata/). Don't worry. The quiz results won't impact your grade. \n",
    "\n",
    "Included below is code that fetches the answers to the questions and provides the results in a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_answers_json = 'https://raw.githubusercontent.com/pixelastic/pokemonorbigdata/master/app/questions.json'\n",
    "df_all = pd.read_json(quiz_answers_json)\n",
    "# Pokémon answers\n",
    "df_all[df_all['type'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big data answers\n",
    "df_all[df_all['type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.2\n",
    "\n",
    "In the next part of the assignment, you will populate the items with categories for our list. The first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*, provides list of categories and subcategories for the big data stack. We will use these categories as a starting point, but will not constrain ourselves to them. \n",
    "\n",
    "When creating categories, avoid deeply nested layers of categories and subcategories. At most, define a top-level category with multiple subcategories. We will start with the following high-level categories and subcategories. \n",
    "\n",
    "***Categories***\n",
    "\n",
    "We will use the disutils trove classification convention defined in [PEP 301](https://www.python.org/dev/peps/pep-0301/) when defining a category with a subcategory. This convention uses the double-colon (\"::\") to separate a category and subcategory. The following is an example of categories and subcategories as defined in the first chapter of the textbook, *Big Data Science & Analytics: A Hands-On Approach*. \n",
    "\n",
    "- Batch Analysis :: DAG\n",
    "- Batch Analysis :: Machine Learning\n",
    "- Batch Analysis :: MapReduce\n",
    "- Batch Analysis :: Script\n",
    "- Batch Analysis :: Search\n",
    "- Batch Analysis :: Workflow Scheduling\n",
    "- Data Access Connector :: Custom Connectors\n",
    "- Data Access Connector :: Publish-Subscribe\n",
    "- Data Access Connector :: Queues\n",
    "- Data Access Connector :: SQL\n",
    "- Data Access Connector :: Source-Sink\n",
    "- Data Storage :: Distributed File System\n",
    "- Data Storage :: NoSQL\n",
    "- Deployment :: NoSQL\n",
    "- Deployment :: SQL\n",
    "- Deployment :: Visualization Frameworks\n",
    "- Deployment :: Web Frameworks\n",
    "- Interactive Querying :: Analytic SQL\n",
    "- Real-Time Analysis :: In-Memory\n",
    "- Real-Time Analysis :: Stream Processing\n",
    "\n",
    "Below is a list containing categories and suggested starting points for research. Fill in a least two items from each of the suggested categories. Create at least one category that is not listed and add two items to that category. \n",
    "\n",
    "* AI and Machine Learning\n",
    "    * Apache Spark's MLlib\n",
    "    * H2O\n",
    "    * Tensorflow\n",
    "* Batch Processing\n",
    "    * Apache\n",
    "    * Apache Spark\n",
    "    * Dask\n",
    "    * MapReduce\n",
    "* Cloud and Data Platforms\n",
    "    * Amazon Web Services\n",
    "    * Cloudera Data Platform\n",
    "    * Google Cloud Platform\n",
    "    * Microsoft Azure\n",
    "* Container Engines and Orchestration\n",
    "    * Docker\n",
    "    * Docker Swarm\n",
    "    * Kubernetes\n",
    "    * Podman\n",
    "* Data Storage :: Block Storage\n",
    "    * Amazon EBS\n",
    "    * OpenEBS\n",
    "* Data Storage :: Cluster Storage\n",
    "    * Ceph\n",
    "    * HDFS\n",
    "* Data Storage :: Object Storage\n",
    "    * Amazon S3\n",
    "    * Minio\n",
    "* Data Transfer Tools\n",
    "    * Apache Sqoop\n",
    "* Full-Text Search\n",
    "    * Apache Solr\n",
    "    * Elasticsearch\n",
    "* Interactive Query\n",
    "    * Apache Hive\n",
    "    * Google Big Query\n",
    "    * Spark SQL\n",
    "* Message Queues\n",
    "    * Apache Kafka\n",
    "    * RabbitMQ\n",
    "* NoSQL :: Document Databases\n",
    "    * CouchDB\n",
    "    * Google Firestore\n",
    "    * MongoDB\n",
    "* NoSQL :: Graph Databases\n",
    "    * DGraph\n",
    "    * Neo4j\n",
    "* NoSQL :: Key-Value Databases\n",
    "    * Amazon DynamoDB\n",
    "* NoSQL :: Time-Series Databases\n",
    "    * TSDB\n",
    "* Serverless Functions\n",
    "    * AWS Lambda\n",
    "    * OpenFaaS\n",
    "* Stream Processing\n",
    "    * Apache Spark's Structured Streaming\n",
    "    * Apache Storm\n",
    "    * Google Dataflow\n",
    "* Visualization Frameworks\n",
    "    * Apache Superset\n",
    "    * Redash\n",
    "* Workflow Engine\n",
    "    * Apache Airflow\n",
    "    * Google Cloud Composer\n",
    "    * Oozie\n",
    "    \n",
    "We populate the list items using the `ListItem` class, defined below. The following is a description of the `ListItem` fields. \n",
    "\n",
    "**name**\n",
    "\n",
    "The proper name of the list item\n",
    "\n",
    "**website**\n",
    "\n",
    "Link to the item's website.  Include `http://` or `https://` in the link. \n",
    "\n",
    "**category**\n",
    "\n",
    "Category and optional subcategory for the item. \n",
    "\n",
    "**short_description**\n",
    "\n",
    "Provide a short, one to two-sentence description of the item. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ListItem:\n",
    "    name: str\n",
    "    website: str\n",
    "    category: str\n",
    "    short_description: str\n",
    "    \n",
    "all_items = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of creating the entry for AWS as a seperate variable and then adding it to the `all_items` set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws = ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    ")\n",
    "\n",
    "all_items.add(aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add an item to the list directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items.remove(aws)\n",
    "all_items.add(ListItem(\n",
    "    'Amazon Web Services',\n",
    "    'https://aws.amazon.com/',\n",
    "    'Cloud and Data Platforms',\n",
    "    \"\"\"Provides on-demand cloud computing platforms and APIs to individuals, \n",
    "    companies, and governments, on a metered pay-as-you-go basis.\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in a least two items from each of the suggested categories. \n",
    "\n",
    "# TODO: Create at least one category that is not listed and add two items to that category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2.3 (Optional)\n",
    "\n",
    "Use the `all_items` data to create Markdown output that mirrors the output of [Awesome Python](https://raw.githubusercontent.com/vinta/awesome-python/master/README.md). You can use the `jinja2` template engine to complete this task. This part of the assignment is entirely optional and is not graded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "\n",
    "template = jinja2.Template(\"\"\"\n",
    "# Awesome Big Data\n",
    "\n",
    "A curated list of awesome big data frameworks, libraries, software and resources.\n",
    "\n",
    "Inspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
