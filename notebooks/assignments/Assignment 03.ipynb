{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce Patterns in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3\n",
    "\n",
    "This assignment will introduce you to the MapReduce programming paradigm using Python and Apache Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global imports\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the following text snippets in parts of your assignment. These values were generated from https://slipsum.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"\n",
    "    Well, the way they make shows is, they make one show. \n",
    "    That show's called a pilot. \n",
    "    Then they show that show to the people who make shows, \n",
    "    and on the strength of that one show they decide \n",
    "    if they're going to make more shows. Some pilots \n",
    "    get picked and become television programs. \n",
    "    Some don't, become nothing. \n",
    "    She starred in one of the ones that became nothing.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "    You think water moves fast? You should see ice. \n",
    "    It moves like it has a mind. \n",
    "    Like it knows it killed the world once and got a taste for murder. \n",
    "    After the avalanche, it took us a week to climb out. \n",
    "    Now, I don't know exactly when we turned on each other, \n",
    "    but I know that seven of us survived the slide... \n",
    "    and only five made it out. Now we took an oath, that I'm breaking now. \n",
    "    We said we'd say it was the snow that killed the other two, but it wasn't. \n",
    "    Nature is lethal but it doesn't hold a candle to man.\n",
    "\"\"\"\n",
    "\n",
    "text3 = \"\"\"\n",
    "    Now that we know who you are, I know who I am. I'm not a mistake! \n",
    "    It all makes sense! In a comic, you know how you can tell who the arch-villain's going to be? \n",
    "    He's the exact opposite of the hero. And most times they're friends, like you and me! \n",
    "    I should've known way back when... You know why, David? Because of the kids. \n",
    "    They called me Mr Glass.\n",
    "\"\"\"\n",
    "\n",
    "text4 = \"\"\"\n",
    "    Your bones don't break, mine do. That's clear. \n",
    "    Your cells react to bacteria and viruses differently than mine. \n",
    "    You don't get sick, I do. That's also clear. \n",
    "    But for some reason, you and I react the exact same way to water. \n",
    "    We swallow it too fast, we choke. We get some in our lungs, we drown. \n",
    "    However unreal it may seem, we are connected, you and I. \n",
    "    We're on the same curve, just on opposite ends.\n",
    "\"\"\"\n",
    "\n",
    "documents = [text1, text2, text3, text4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.1\n",
    " \n",
    "In the first part of the assignment, you implement map and reduce functions that count the occurrences of words in a collection of documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Assignment 3.1.a\n",
    "\n",
    "In this part, you will implement a function that takes a document and outputs word/count tuple pairs. You will then apply that function across multiple documents using Python's built-in `map` function. \n",
    "\n",
    "The `map` function applies a function to every item in an input list of values. Technically, the input can be any Python iterable, but for the sake of simplicity, assume it is a list of values.  The primary reason to use the `map` function is that it is simple to parallelize its execution across multiple processes and computers. \n",
    "\n",
    "The following is a simple example of using Python's `multiprocessing` library to run a `map` function in five parallel processes. \n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def square_number(x):\n",
    "    return x*x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with Pool(5) as p:\n",
    "        values = [1, 2, 3, 4, 5]\n",
    "        print(p.map(square_number, values))\n",
    "```\n",
    "\n",
    "The above code prints the following result to standard output.\n",
    "\n",
    "```\n",
    "[1, 4, 9, 16, 25]\n",
    "```\n",
    "\n",
    "This is the same example that uses Python's built-in `map` function. This function does not take advantage of parallel execution. \n",
    "\n",
    "```python\n",
    "def square_number(x):\n",
    "    return x*x\n",
    "\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "results = map(square_number, numbers)\n",
    "print(list(results))\n",
    "```\n",
    "\n",
    "A partially implemented version of the `word_count_map_function` is included below. Fill in the missing details. The `remove_punctuation` is provided as a way of removing punctuation from the input text. You can use this function in the implementation of the `word_count_map_function`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple function to remove punctuation from text\n",
    "\n",
    "    :param text: text to remove punctuation from\n",
    "    :return: text with punctuation removed\n",
    "    \"\"\"\n",
    "    return ''.join([\n",
    "        character\n",
    "        for character in text\n",
    "        if character not in string.punctuation\n",
    "    ])\n",
    "\n",
    "def word_count_map_function(text: str) -> list[tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Simple map function that takes text and outputs tuples\n",
    "    of words and counts\n",
    "    \n",
    "    :param text: text to convert into word/count tuples\n",
    "    :return: A list of tuples with word and count\n",
    "    \"\"\"\n",
    "    # TODO: Implement the word count map function\n",
    "\n",
    "    # Step 1: Remove punctuation from text\n",
    "\n",
    "    # Step 2: Convert text to lower case\n",
    "\n",
    "    # Step 3: Split the text by spaces to convert into words\n",
    "\n",
    "    # Step 4: Return a list containing a dictionary of words and counts\n",
    "    \n",
    "    # This line is here as a placeholder. Replace with your own code\n",
    "    words = []\n",
    "\n",
    "    # This line is here as a placeholder. Replace with your own code\n",
    "    word_count_pairs = [('word1', 1), ('word2', 1)]\n",
    "    return word_count_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is example output of the function applied to `text1`\n",
    "```python\n",
    ">>> word_count_map_function(text1)\n",
    "\n",
    "[('well', 1),\n",
    " ('the', 1),\n",
    " ('way', 1),\n",
    " ('they', 1),\n",
    " ('make', 1),\n",
    " ('shows', 1),\n",
    "    ...\n",
    " ('ones', 1),\n",
    " ('that', 1),\n",
    " ('became', 1),\n",
    " ('nothing', 1)]\n",
    "```\n",
    "\n",
    "Note: The count for each of these examples is *1* since the mapper did not perform a reduce task (i.e. combine key/value pairs). In practice,  mappers may perform an internal reduce before sending along data to the global reduce task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.1.b\n",
    "\n",
    "A MapReduce program usually contains multiple steps. Each *map* function runs in parallel and outputs a key/value pair. In the case of the word count task, the word is the key and the count is the value. The *reduce* function sorts each of the *map* outputs by the key (i.e. word) and then combine the results. The *reduce* phase is more computationally expensive as it involves sorting and combining data from multiple *map* functions. \n",
    "\n",
    "In this part, you will implement a function that combines the word/count pairs from the previous step. You will then apply that function across all of the *map* outputs using Python's *reduce* function. Previously, *reduce* was included in Python's built-in library, but now it is included in the *functools* library.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses the *reduce* function to combine two lists.  \n",
    "\n",
    "```python\n",
    "list1 = [1, 2, 3, 4, 5]\n",
    "list2 = [6, 7, 8, 9, 10]\n",
    "\n",
    "lists = [list1, list2]\n",
    "\n",
    "def merge_lists_reduce_function(x, y):\n",
    "    return x + y\n",
    "\n",
    "merged_values = reduce(merge_lists_reduce_function, lists)\n",
    "merged_values\n",
    "```\n",
    "This produces the output:\n",
    "\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "```\n",
    "\n",
    "We can use the *reduce* function a second time to add the numbers in the combined list. \n",
    "\n",
    "```\n",
    "reduce(add, merged_values)\n",
    "```\n",
    "This returns the result:\n",
    "\n",
    "```\n",
    "55\n",
    "```\n",
    "\n",
    "A partially implemented version of the `word_count_reduce_function` is included below. Fill in the missing details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_reduce_function(key_value_pairs):\n",
    "    result = dict()\n",
    "    \n",
    "    # TODO: Implement code to return list containing word/count pairs\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses the functions defined previously to run the MapReduce job. The `counted_words_dict` variable should be a dictionary where the words are the keys and the total count for each word are the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply the `word_count_map_function` to each the documents\n",
    "map_output_values = map(word_count_map_function, documents)\n",
    "# Step 2: Merge the outputs of the map function into a single list\n",
    "merged_output_values = reduce(merge_lists_reduce_function, map_output_values)\n",
    "# Step 3: Apply the `word_count_reduce_function` to create a single word/count dictionary\n",
    "counted_words_dict = word_count_reduce_function(merged_output_values)\n",
    "counted_words_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.2\n",
    "\n",
    "The second part of the assignment reproduces the word-count task using  [Apache Spark's resilient distributed dataset (RDD)](https://spark.apache.org/docs/latest/rdd-programming-guide.html). This abstraction allows MapReduce code to run in parallel in the memory of multiple computers.  \n",
    "\n",
    "The following code initializes the Spark application. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DSC 400 Assignment 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code snippet loads the previously defined `documents` and creates a Spark RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_rdd = spark_context.parallelize(documents)\n",
    "\n",
    "print(\"Number of Partitions: \"+str(documents_rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark waits until there is output to execute this code. The `collect` method causes Spark to collect all the elements in the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_rdd_collect = documents_rdd.collect()\n",
    "print(documents_rdd_collect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `flatMap` method applies the `word_count_map_function` to each the documents. Unlike the `map` method, `flatMap` flattens the results into a single collection. Using `flatMap` means there is no need for a second step to merge the multiple results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_output_rdd = documents_rdd.flatMap(word_count_map_function)\n",
    "map_output_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `reduceByKey` method combines the results by the key (i.e. word). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_count_rdd = map_output_rdd.reduceByKey(add)\n",
    "word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asssignment 3.2.a\n",
    "\n",
    "The `word_count_rdd` variable is an RDD that contains combined word/count pairs. Using this as a starting point, apply [filter](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.filter.html) to create a new RDD that contains entries with word counts greater than four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate `counts_gt_four_rdd` from `word_count_rdd`\n",
    "counts_gt_four_rdd = None\n",
    "counts_gt_four_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asssignment 3.2.b\n",
    "\n",
    "Using [sortBy](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sortBy.html), create an RDD sorted by word count. The RDD should be sorted from greatest to least (i.e. descending values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate `sorted_word_count_rdd` from `word_count_rdd`\n",
    "sorted_word_count_rdd = None\n",
    "sorted_word_count_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3.3\n",
    "\n",
    "[Apache Spark's datasets and dataframes](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes) provides a programming interface that abstracts away the underlying details associated with MapReduce or RDDs. In this part of the assignment, you will use Spark's dataframes to perform a word count. You will perform additional operations such as filtering and sorting. \n",
    "\n",
    "The following code creates a [Spark dataframe](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) from the previously defined `word_count_rdd`.  The `printSchema` method prints the schema for the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_columns = [\"word\", \"count\"]\n",
    "word_count_df = spark.createDataFrame(\n",
    "    data=word_count_rdd, \n",
    "    schema=word_count_columns\n",
    ")\n",
    "word_count_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show` method will print the dataframe as a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.3.a\n",
    "\n",
    "Use the [filter](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html) method to create a new dataframe with word counts greater than four. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create `word_count_filtered` from `word_count_df`\n",
    "\n",
    "word_count_filtered = None\n",
    "word_count_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.3.b\n",
    "\n",
    "Use the [sort](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sort.html) method to create a new dataframe sorted by word count. Sort the word counts from greatest to least (i.e. descending)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create `word_count_sorted` from `word_count_df`\n",
    "\n",
    "word_count_sorted = None\n",
    "word_count_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 3.3.c\n",
    "\n",
    "Use the [limit](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.limit.html)\n",
    "method to create a new dataframe that contains the top ten word count values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create `word_count_top_10` from `word_count_sorted`\n",
    "\n",
    "word_count_top_10 = None\n",
    "word_count_top_10.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}